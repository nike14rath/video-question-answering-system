# ğŸ¥ Video Question Answering System (Streamlit + Ollama)

This project implements an **end-to-end Video Question Answering (Video-QA) system** that allows users to ask questions about tutorial videos and receive **precise answers with video names and timestamps**.

The entire system works **offline**, using **Whisper** for transcription, **Ollama** for embeddings and LLM inference, and **Streamlit** for the user interface.

---

## ğŸš€ Features

- ğŸ§ Automatically convert videos to audio
- ğŸ“ Transcribe and translate audio using Whisper
- âœ‚ï¸ Chunk transcripts with precise timestamps
- ğŸ§  Generate semantic embeddings using **bge-m3**
- ğŸ” Perform similarity search over video content
- ğŸ¤– Answer questions using an LLM (**LLaMA**)
- ğŸ–¥ï¸ Interactive Streamlit web interface
- ğŸ“ Timestamp-based video guidance

---

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ app.py                                   # Streamlit web application
â”œâ”€â”€ step1_process_vedio.py                   # Video â†’ Audio conversion
â”œâ”€â”€ step2_creating_chunks.py                 # Transcription & chunk creation
â”œâ”€â”€ step3_saving_the_chucks_with_embeddings.py
â”œâ”€â”€ step4_reading.py                         # Create embedding DataFrame
â”œâ”€â”€ step5_incoming_query.py                  # CLI-based QA (optional)
â”‚
â”œâ”€â”€ Vedios/                                  # Input video files
â”œâ”€â”€ audios/                                  # Extracted audio files
â”œâ”€â”€ Json/                                    # Transcription JSON files
â”œâ”€â”€ jsons/                                   # JSON files with embeddings
â”œâ”€â”€ Dataframe_with_embeddings.joblib         # Vector store
â”œâ”€â”€ prompt.txt                               # Prompt sent to LLM
â””â”€â”€ response.txt                             # LLM response output
```

---

## ğŸ§© Pipeline Overview

```text
Videos
  â†“
Audio Extraction (FFmpeg)
  â†“
Transcription & Translation (Whisper)
  â†“
Timestamped Text Chunks
  â†“
Embeddings (Ollama â€“ bge-m3)
  â†“
Similarity Search
  â†“
LLM Answer Generation
  â†“
Streamlit UI Output
```

---

## ğŸ› ï¸ Step-by-Step File Explanation

### ğŸ”¹ step1_process_vedio.py
**Purpose:** Convert videos into audio files

- Reads videos from the `Vedios/` directory
- Extracts tutorial number and title
- Converts videos to `.mp3` using FFmpeg

---

### ğŸ”¹ step2_creating_chunks.py
**Purpose:** Transcribe and chunk audio

- Uses Whisper (`medium` model)
- Translates Hindi â†’ English
- Stores text with start and end timestamps
- Outputs structured JSON files

---

### ğŸ”¹ step3_saving_the_chucks_with_embeddings.py
**Purpose:** Generate embeddings

- Uses Ollama embedding API (`bge-m3`)
- Batch embedding with fallback mechanism
- Filters invalid or very short text chunks
- Saves embeddings into JSON files

---

### ğŸ”¹ step4_reading.py
**Purpose:** Create a unified vector store

- Loads all embedded JSON files
- Combines chunks into a single Pandas DataFrame
- Saves the DataFrame as `Dataframe_with_embeddings.joblib`

---

### ğŸ”¹ step5_incoming_query.py (Optional)
**Purpose:** CLI-based question answering

- Accepts user queries from terminal
- Finds relevant video chunks
- Generates LLM-based answers
- Saves prompt and response to files

---

### ğŸ”¹ app.py (Main Application)
**Purpose:** Streamlit web interface

- Accepts user questions
- Computes semantic similarity
- Retrieves top matching video segments
- Generates LLM-based answers
- Displays results in a clean, interactive UI

---

## ğŸ“¦ Requirements

### System Requirements
- Python 3.9+
- FFmpeg
- Ollama (running locally)
- GPU recommended (for Whisper)

### Python Dependencies

```bash
pip install streamlit joblib numpy pandas scikit-learn requests torch whisper
```

---

## ğŸ§  Ollama Setup

```bash
ollama serve
ollama pull bge-m3
ollama pull llama3.2
```

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Prepare the Data (run once)

```bash
python step1_process_vedio.py
python step2_creating_chunks.py
python step3_saving_the_chucks_with_embeddings.py
python step4_reading.py
```

### 2ï¸âƒ£ Launch the Streamlit App

```bash
python -m streamlit run app.py
```

Open in your browser:

```text
http://localhost:8501
```

---

## ğŸ¯ Usage Example

```text
Question:
What are semantic HTML tags?

Output:
â€¢ Video: HTML_Tutorial_11.mp3
â€¢ Timestamp: 04:18 â€“ 06:42
â€¢ Explanation: Semantic tags define the meaning of HTML elements...
```

---

## ğŸ” Notes & Limitations

- Answers are strictly based on video content
- Unrelated questions are rejected
- Ollama must be running locally
- Embeddings must be regenerated if videos change

---

## ğŸŒ± Future Improvements

- FAISS integration for faster search
- Clickable video timestamps
- Multi-video filtering
- Chat history & conversational memory
- Cloud deployment
- Enhanced UI & analytics

---

## ğŸ™Œ Acknowledgements

- OpenAI Whisper
- Ollama
- Streamlit
- Scikit-learn
